{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bebin11/AI_WS/blob/main/Workshop_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUuuYhq3oMNS",
        "outputId": "680e8b52-8dba-4de3-899a-321713581a8d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqtL0xs7t6IS",
        "outputId": "101f6363-9e91-4b60-a9b1-9d54a0cca54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info:\n",
            "Shape: (50000, 2)\n",
            "\n",
            "Column names: ['review', 'sentiment']\n",
            "\n",
            "First review example:\n",
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ...\n",
            "\n",
            "Sentiment: positive\n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "path = \"/content/drive/MyDrive/IMDB_Dataset.csv\"\n",
        "df = pd.read_csv(path, encoding=\"utf-8\", engine=\"python\")\n",
        "length = len(df['review'])\n",
        "print(\"Dataset info:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst review example:\\n{df['review'].iloc[0][:500]}...\")\n",
        "print(f\"\\nSentiment: {df['sentiment'].iloc[0]}\")\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(df['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1:**\n",
        "1. Load the dataset and preprocess the reviews.\n",
        "\n",
        "    a. Convert all text to lowercase.\n",
        "\n",
        "    b. Remove non-alphabetic characters (punctuation).\n",
        "\n",
        "    c. Tokenize the reviews and remove common stopwords.\n",
        "\n",
        "    d. Apply stemming to reduce words to their root form.\n",
        "\n",
        "2. Split the dataset into training and testing sets (80% training, 20% testing).\n",
        "\n",
        "3. Use a Naive Bayes classifier to classify the reviews into positive and negative categories.\n",
        "\n",
        "    a. Implement a Bag-of-Words model using CountVectorizer.\n",
        "\n",
        "    b. Train the Naive Bayes classifier using the training set."
      ],
      "metadata": {
        "id": "DQ1NeyTqzb-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all reviews to lowercase\n",
        "df['review'] = df['review'].str.lower()\n",
        "print(\"After converting to lowercase:\")\n",
        "print(df['review'].iloc[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjkv-9rMzEh6",
        "outputId": "091a1c3a-b49f-4498-bac7-3753d91d0cc8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After converting to lowercase:\n",
            "one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.<br /><br />the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word.<br /><br />it is called oz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove non-alphabetic characters and extra whitespace\n",
        "df['review_clean'] = df['review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "df['review_clean'] = df['review_clean'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "print(\"After removing non-alphabetic characters:\")\n",
        "print(df['review_clean'].iloc[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9DUonhdz38W",
        "outputId": "3d506d77-131a-44ef-f86d-840e672af427"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing non-alphabetic characters:\n",
            "one of the other reviewers has mentioned that after watching just oz episode youll be hooked they are right as this is exactly what happened with mebr br the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordbr br it is called oz as that is the nickname g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "df['review_tokens'] = df['review_clean'].apply(lambda x: [word for word in x.split() if word not in stop_words])\n",
        "\n",
        "print(\"After tokenization and removing stopwords:\")\n",
        "print(f\"First review tokens (first 20): {df['review_tokens'].iloc[0][:20]}\")\n",
        "print(f\"Number of tokens in first review: {len(df['review_tokens'].iloc[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zssZlbWC0JX4",
        "outputId": "8cf473bc-0e85-490b-95c4-5e119ad7a954"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After tokenization and removing stopwords:\n",
            "First review tokens (first 20): ['one', 'reviewers', 'mentioned', 'watching', 'oz', 'episode', 'youll', 'hooked', 'right', 'exactly', 'happened', 'mebr', 'br', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes']\n",
            "Number of tokens in first review: 170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to tokens\n",
        "df['review_stemmed'] = df['review_tokens'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "\n",
        "print(\"After stemming:\")\n",
        "print(f\"First review stemmed tokens (first 20): {df['review_stemmed'].iloc[0][:20]}\")\n",
        "\n",
        "# Join tokens back into strings for the model\n",
        "df['review_processed'] = df['review_stemmed'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "print(f\"\\nProcessed review (first 500 chars):\")\n",
        "print(df['review_processed'].iloc[0][:500])"
      ],
      "metadata": {
        "id": "pzD8MPuN0onj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947a7dcd-a8ae-415c-d762-c22753905502"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming:\n",
            "First review stemmed tokens (first 20): ['one', 'review', 'mention', 'watch', 'oz', 'episod', 'youll', 'hook', 'right', 'exactli', 'happen', 'mebr', 'br', 'first', 'thing', 'struck', 'oz', 'brutal', 'unflinch', 'scene']\n",
            "\n",
            "Processed review (first 500 chars):\n",
            "one review mention watch oz episod youll hook right exactli happen mebr br first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use wordbr br call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare the features (X) and labels (y)\n",
        "X = df['review_processed']  # Processed text\n",
        "y = df['sentiment']  # Labels (positive/negative)\n",
        "\n",
        "# Convert labels to binary (1 for positive, 0 for negative)\n",
        "y_binary = y.map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "print(\"=== DATA SPLITTING RESULTS ===\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTesting set class distribution:\")\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "di8G_pKB0wL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb288aa-9caa-40e5-c3ee-3c55370850ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA SPLITTING RESULTS ===\n",
            "Total samples: 50000\n",
            "Training samples: 40000 (80.0%)\n",
            "Testing samples: 10000 (20.0%)\n",
            "\n",
            "Training set class distribution:\n",
            "sentiment\n",
            "1    20000\n",
            "0    20000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Testing set class distribution:\n",
            "sentiment\n",
            "0    5000\n",
            "1    5000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000)  # Use top 5000 features\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform test data\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"=== BAG-OF-WORDS MODEL CREATED ===\")\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "print(f\"Training data shape: {X_train_bow.shape}\")\n",
        "print(f\"Testing data shape: {X_test_bow.shape}\")\n",
        "print(f\"\\nSample feature names (first 20):\")\n",
        "print(list(vectorizer.get_feature_names_out())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvlwwkEwEwjo",
        "outputId": "c756d875-d2fd-435b-da5b-afd54deb13a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1787587611.py\", line 10, in <cell line: 0>\n",
            "    X_test_bow = vectorizer.transform(X_test)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\", line 1421, in transform\n",
            "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\", line None, in _count_vocab\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1716, in getframeinfo\n",
            "    start = lineno - 1 - context//2\n",
            "            ~~~~~~~^~~\n",
            "TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1787587611.py\", line 10, in <cell line: 0>\n",
            "    X_test_bow = vectorizer.transform(X_test)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\", line 1421, in transform\n",
            "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\", line None, in _count_vocab\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1716, in getframeinfo\n",
            "    start = lineno - 1 - context//2\n",
            "            ~~~~~~~^~~\n",
            "TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1787587611.py\", line 10, in <cell line: 0>\n",
            "    X_test_bow = vectorizer.transform(X_test)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\", line 1421, in transform\n",
            "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\", line None, in _count_vocab\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1716, in getframeinfo\n",
            "    start = lineno - 1 - context//2\n",
            "            ~~~~~~~^~~\n",
            "TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "nb_classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "print(\"=== NAIVE BAYES CLASSIFIER TRAINED ===\")\n",
        "print(f\"Classifier type: {type(nb_classifier).__name__}\")\n",
        "print(f\"Training completed successfully!\")"
      ],
      "metadata": {
        "id": "6zlJ-31C0fHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b99a01-1a55-4325-9d92-25d28ef2ab4e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NAIVE BAYES CLASSIFIER TRAINED ===\n",
            "Classifier type: MultinomialNB\n",
            "Training completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2:**\n",
        "1. Evaluate the performance of the model using the following metrics:\n",
        "\n",
        "    a. Accuracy\n",
        "\n",
        "    b. Precision, Recall, and F1-score\n",
        "\n",
        "    c. Confusion Matrix\n",
        "\n",
        "    d. ROC-AUC Score"
      ],
      "metadata": {
        "id": "jabt1hxXFhUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TLPy_nhdE0UH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions and prediction probabilities\n",
        "y_pred = nb_classifier.predict(X_test_bow)\n",
        "y_pred_proba = nb_classifier.predict_proba(X_test_bow)[:, 1]  # Probabilities for positive class\n",
        "\n",
        "print(\"=== PREDICTIONS READY FOR EVALUATION ===\")\n",
        "print(f\"Shape of predictions: {y_pred.shape}\")\n",
        "print(f\"Shape of prediction probabilities: {y_pred_proba.shape}\")"
      ],
      "metadata": {
        "id": "xmStz0-HIO2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== ACCURACY ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Accuracy percentage: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "D1CZkFiqITmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== PRECISION, RECALL, AND F1-SCORE ===\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "id": "LQpw6wppIWBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"=== CONFUSION MATRIX ===\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix - Naive Bayes Classifier')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate confusion matrix metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"\\nFalse Positive Rate: {fp/(fp+tn):.4f}\")\n",
        "print(f\"False Negative Rate: {fn/(fn+tp):.4f}\")"
      ],
      "metadata": {
        "id": "7TAXoX6YIyrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"=== ROC-AUC SCORE ===\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Find optimal threshold (Youden's J statistic)\n",
        "youden_j = tpr - fpr\n",
        "optimal_idx = np.argmax(youden_j)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(f\"\\nOptimal Threshold (Youden's J): {optimal_threshold:.4f}\")\n",
        "print(f\"TPR at optimal threshold: {tpr[optimal_idx]:.4f}\")\n",
        "print(f\"FPR at optimal threshold: {fpr[optimal_idx]:.4f}\")"
      ],
      "metadata": {
        "id": "bFO6uZPBI00_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_review(review):\n",
        "    review = review.lower()\n",
        "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
        "    review = re.sub(r'\\s+', ' ', review).strip()\n",
        "    tokens = [word for word in review.split() if word not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "sample_reviews = [\n",
        "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
        "    \"Terrible film. Waste of time and money. Don't watch it.\",\n",
        "    \"The acting was good but the plot was confusing.\",\n",
        "    \"One of the best movies I've seen this year, highly recommended!\",\n",
        "    \"Boring and predictable, I fell asleep halfway through.\"\n",
        "]\n",
        "\n",
        "print(\"=== SAMPLE REVIEW PREDICTIONS ===\")\n",
        "for i, review in enumerate(sample_reviews, 1):\n",
        "    processed_review = preprocess_review(review)\n",
        "    review_vectorized = vectorizer.transform([processed_review])\n",
        "    prediction = nb_classifier.predict(review_vectorized)[0]\n",
        "    probability = nb_classifier.predict_proba(review_vectorized)[0][1]\n",
        "\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    print(f\"\\nReview {i}:\")\n",
        "    print(f\"Text: {review[:60]}...\")\n",
        "    print(f\"Predicted: {sentiment}\")\n",
        "    print(f\"Probability (Positive): {probability:.4f}\")"
      ],
      "metadata": {
        "id": "BeCuDodsI4Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/breast-cancer-wisconsin.data.csv\")"
      ],
      "metadata": {
        "id": "U6fDxbpII8XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATASET LOADED ===\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "jLHZs-H-Lf7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the extra 'index' column if it exists\n",
        "if 'index' in df.columns:\n",
        "    df = df.drop(columns=['index'])\n",
        "    print(f\"Removed 'index' column. New shape: {df.shape}\")\n",
        "\n",
        "# Now assign proper column names\n",
        "column_names = [\n",
        "    'ID_Number',\n",
        "    'Clump_Thickness',\n",
        "    'Uniformity_Cell_Size',\n",
        "    'Uniformity_Cell_Shape',\n",
        "    'Marginal_Adhesion',\n",
        "    'Single_Epithelial_Size',\n",
        "    'Bare_Nuclei',\n",
        "    'Bland_Chromatin',\n",
        "    'Normal_Nucleoli',\n",
        "    'Mitoses',\n",
        "    'Class'\n",
        "]\n",
        "\n",
        "# Assign column names\n",
        "df.columns = column_names\n",
        "\n",
        "print(\"\\n=== DATASET WITH PROPER COLUMN NAMES ===\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "print(df.head(3))"
      ],
      "metadata": {
        "id": "58UzIoAUcRuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA CLEANING ===\")\n",
        "\n",
        "# Check data types\n",
        "print(\"Data types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Convert 'Bare_Nuclei' to numeric (it might have '?' values)\n",
        "print(\"\\nConverting 'Bare_Nuclei' to numeric...\")\n",
        "df['Bare_Nuclei'] = pd.to_numeric(df['Bare_Nuclei'], errors='coerce')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_clean = df.dropna()\n",
        "print(f\"\\nOriginal shape: {df.shape}\")\n",
        "print(f\"After dropping missing values: {df_clean.shape}\")\n",
        "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]}\")\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\n=== CLASS DISTRIBUTION ===\")\n",
        "class_dist = df_clean['Class'].value_counts().sort_index()\n",
        "print(class_dist)\n",
        "\n",
        "# Map class values\n",
        "class_mapping = {2: 'Benign', 4: 'Malignant'}\n",
        "for class_val, count in class_dist.items():\n",
        "    label = class_mapping.get(class_val, f'Unknown ({class_val})')\n",
        "    percentage = (count / len(df_clean)) * 100\n",
        "    print(f\"Class {class_val} ({label}): {count} samples ({percentage:.1f}%)\")"
      ],
      "metadata": {
        "id": "xa54dsCRcYDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare features (X) and target (y)\n",
        "# Drop ID_Number as it's not a feature\n",
        "X = df_clean.drop(columns=['ID_Number', 'Class'])\n",
        "y = df_clean['Class']\n",
        "\n",
        "# Convert class to binary: 0 for Benign (2), 1 for Malignant (4)\n",
        "y_binary = y.replace({2: 0, 4: 1})\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "print(\"=== DATA SPLITTING COMPLETED ===\")\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "print(f\"Benign (0): {sum(y_train == 0)} samples\")\n",
        "print(f\"Malignant (1): {sum(y_train == 1)} samples\")\n",
        "\n",
        "print(f\"\\nTesting set class distribution:\")\n",
        "print(f\"Benign (0): {sum(y_test == 0)} samples\")\n",
        "print(f\"Malignant (1): {sum(y_test == 1)} samples\")"
      ],
      "metadata": {
        "id": "iSPkYoEckg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Apply a Wrapper Method**\n",
        "\n",
        "1. Use Recursive Feature Elimination (RFE) with a Logistic Regression model to perform feature selection:\n",
        "\n",
        "• Select the top 5 features that contribute the most to predicting the target variable.\n",
        "\n",
        "• Visualize the ranking of features.\n",
        "\n",
        "2. Train the Logistic Regression model using only the selected features."
      ],
      "metadata": {
        "id": "z8P9Y_fLkqD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"=== APPLYING RECURSIVE FEATURE ELIMINATION (RFE) ===\")\n",
        "\n",
        "# Standardize the features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Initialize RFE to select top 5 features\n",
        "rfe = RFE(estimator=logreg, n_features_to_select=5, step=1)\n",
        "\n",
        "# Fit RFE\n",
        "rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"RFE fitting completed!\")\n",
        "print(f\"Selected {sum(rfe.support_)} features out of {X.shape[1]}\")"
      ],
      "metadata": {
        "id": "mxn-fdG7kjvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get selected features\n",
        "selected_features = X.columns[rfe.support_].tolist()\n",
        "print(\"\\n=== TOP 5 SELECTED FEATURES ===\")\n",
        "print(\"Selected features:\")\n",
        "for i, feature in enumerate(selected_features, 1):\n",
        "    print(f\"{i}. {feature}\")\n",
        "\n",
        "# Get feature rankings\n",
        "feature_rankings = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Ranking': rfe.ranking_,\n",
        "    'Selected': rfe.support_\n",
        "}).sort_values('Ranking')\n",
        "\n",
        "print(\"\\n=== FEATURE RANKINGS ===\")\n",
        "print(feature_rankings)"
      ],
      "metadata": {
        "id": "HRmrj7uRkx6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize feature rankings\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create horizontal bar plot\n",
        "features = feature_rankings['Feature']\n",
        "rankings = feature_rankings['Ranking']\n",
        "colors = ['green' if sel else 'red' for sel in feature_rankings['Selected']]\n",
        "\n",
        "plt.barh(range(len(features)), rankings, color=colors)\n",
        "plt.yticks(range(len(features)), features)\n",
        "plt.xlabel('Ranking (1 = Best)')\n",
        "plt.title('Feature Rankings from Recursive Feature Elimination (RFE)')\n",
        "plt.gca().invert_yaxis()  # Highest ranking at the top\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(rankings):\n",
        "    plt.text(v + 0.1, i, str(v), color='black', va='center')\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a more detailed visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Feature rankings\n",
        "ax1.barh(features, rankings, color=colors)\n",
        "ax1.set_xlabel('Ranking (Lower is better)')\n",
        "ax1.set_title('Feature Rankings from RFE')\n",
        "ax1.invert_yaxis()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Selected vs Not Selected\n",
        "selected_counts = feature_rankings['Selected'].value_counts()\n",
        "labels = ['Selected', 'Not Selected']\n",
        "colors_pie = ['green', 'red']\n",
        "ax2.pie(selected_counts, labels=labels, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title(f'Feature Selection: {selected_counts[True]} out of {len(features)} selected')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IoFeVZnok07S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== TRAINING LOGISTIC REGRESSION WITH SELECTED FEATURES ===\")\n",
        "\n",
        "# Get only the selected features\n",
        "X_train_selected = X_train_scaled[:, rfe.support_]\n",
        "X_test_selected = X_test_scaled[:, rfe.support_]\n",
        "\n",
        "# Train logistic regression on selected features\n",
        "logreg_selected = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg_selected.fit(X_train_selected, y_train)\n",
        "\n",
        "print(\"Model trained with selected features!\")\n",
        "print(f\"Number of features used: {logreg_selected.coef_.shape[1]}\")\n",
        "print(f\"Selected features: {selected_features}\")"
      ],
      "metadata": {
        "id": "pl7FtcOlk3JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Model Evaluation\n",
        "1. Evaluate the model’s performance using the testing set:\n",
        "• Metrics to calculate: Accuracy, Precision, Recall, F1-Score, and ROC-AUC.\n",
        "2. Compare the performance of the model trained on all features versus the model trained on the selected\n",
        "features."
      ],
      "metadata": {
        "id": "o2JHrD95lMmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=== EVALUATING MODEL WITH SELECTED FEATURES ===\")\n",
        "\n",
        "# Make predictions with selected features model\n",
        "y_pred_selected = logreg_selected.predict(X_test_selected)\n",
        "y_pred_proba_selected = logreg_selected.predict_proba(X_test_selected)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "precision_selected = precision_score(y_test, y_pred_selected)\n",
        "recall_selected = recall_score(y_test, y_pred_selected)\n",
        "f1_selected = f1_score(y_test, y_pred_selected)\n",
        "roc_auc_selected = roc_auc_score(y_test, y_pred_proba_selected)\n",
        "\n",
        "print(\"\\n=== PERFORMANCE METRICS (SELECTED FEATURES) ===\")\n",
        "print(f\"Accuracy:  {accuracy_selected:.4f}\")\n",
        "print(f\"Precision: {precision_selected:.4f}\")\n",
        "print(f\"Recall:    {recall_selected:.4f}\")\n",
        "print(f\"F1-Score:  {f1_selected:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_selected:.4f}\")\n",
        "\n",
        "print(\"\\n=== CLASSIFICATION REPORT (SELECTED FEATURES) ===\")\n",
        "print(classification_report(y_test, y_pred_selected,\n",
        "                           target_names=['Benign', 'Malignant']))\n",
        "\n",
        "# Confusion Matrix for selected features\n",
        "cm_selected = confusion_matrix(y_test, y_pred_selected)\n",
        "print(\"\\n=== CONFUSION MATRIX (SELECTED FEATURES) ===\")\n",
        "print(cm_selected)"
      ],
      "metadata": {
        "id": "QVVa116Wk8Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: TRAINING MODEL WITH ALL FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train logistic regression with ALL features\n",
        "logreg_all = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg_all.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions with all features model\n",
        "y_pred_all = logreg_all.predict(X_test_scaled)\n",
        "y_pred_proba_all = logreg_all.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate metrics for all features model\n",
        "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
        "precision_all = precision_score(y_test, y_pred_all)\n",
        "recall_all = recall_score(y_test, y_pred_all)\n",
        "f1_all = f1_score(y_test, y_pred_all)\n",
        "roc_auc_all = roc_auc_score(y_test, y_pred_proba_all)\n",
        "\n",
        "print(\"\\n=== PERFORMANCE METRICS (ALL FEATURES) ===\")\n",
        "print(f\"Accuracy:  {accuracy_all:.4f}\")\n",
        "print(f\"Precision: {precision_all:.4f}\")\n",
        "print(f\"Recall:    {recall_all:.4f}\")\n",
        "print(f\"F1-Score:  {f1_all:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_all:.4f}\")\n",
        "\n",
        "print(\"\\n=== CLASSIFICATION REPORT (ALL FEATURES) ===\")\n",
        "print(classification_report(y_test, y_pred_all,\n",
        "                           target_names=['Benign', 'Malignant']))\n",
        "\n",
        "# Confusion Matrix for all features\n",
        "cm_all = confusion_matrix(y_test, y_pred_all)\n",
        "print(\"\\n=== CONFUSION MATRIX (ALL FEATURES) ===\")\n",
        "print(cm_all)"
      ],
      "metadata": {
        "id": "iGnPr7eflV3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== VISUAL COMPARISON OF MODELS ===\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
        "    'All Features': [accuracy_all, precision_all, recall_all, f1_all, roc_auc_all],\n",
        "    'Selected Features': [accuracy_selected, precision_selected,\n",
        "                         recall_selected, f1_selected, roc_auc_selected]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Difference'] = comparison_df['Selected Features'] - comparison_df['All Features']\n",
        "\n",
        "print(\"\\nComparison Table:\")\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Bar chart comparison\n",
        "ax1 = axes[0, 0]\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, comparison_df['All Features'], width,\n",
        "               label='All Features', color='skyblue', alpha=0.8)\n",
        "bars2 = ax1.bar(x + width/2, comparison_df['Selected Features'], width,\n",
        "               label='Selected Features', color='lightcoral', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Metrics')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Model Performance Comparison')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(comparison_df['Metric'], rotation=45)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 2: Confusion Matrix for Selected Features\n",
        "ax2 = axes[0, 1]\n",
        "sns.heatmap(cm_selected, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Benign', 'Predicted Malignant'],\n",
        "            yticklabels=['Actual Benign', 'Actual Malignant'],\n",
        "            ax=ax2)\n",
        "ax2.set_title('Confusion Matrix (Selected Features)')\n",
        "\n",
        "# Plot 3: Confusion Matrix for All Features\n",
        "ax3 = axes[1, 0]\n",
        "sns.heatmap(cm_all, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['Predicted Benign', 'Predicted Malignant'],\n",
        "            yticklabels=['Actual Benign', 'Actual Malignant'],\n",
        "            ax=ax3)\n",
        "ax3.set_title('Confusion Matrix (All Features)')\n",
        "\n",
        "# Plot 4: ROC Curves Comparison\n",
        "ax4 = axes[1, 1]\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Calculate ROC curves\n",
        "fpr_selected, tpr_selected, _ = roc_curve(y_test, y_pred_proba_selected)\n",
        "fpr_all, tpr_all, _ = roc_curve(y_test, y_pred_proba_all)\n",
        "\n",
        "ax4.plot(fpr_all, tpr_all, color='blue', lw=2,\n",
        "         label=f'All Features (AUC = {roc_auc_all:.3f})')\n",
        "ax4.plot(fpr_selected, tpr_selected, color='red', lw=2,\n",
        "         label=f'Selected Features (AUC = {roc_auc_selected:.3f})')\n",
        "ax4.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
        "ax4.set_xlabel('False Positive Rate')\n",
        "ax4.set_ylabel('True Positive Rate')\n",
        "ax4.set_title('ROC Curves Comparison')\n",
        "ax4.legend(loc='lower right')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Feature Selection Impact Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "my6tyWUxlbPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== INTERPRETATION OF RESULTS ===\")\n",
        "print(\"\\n1. PERFORMANCE COMPARISON:\")\n",
        "for i, row in comparison_df.iterrows():\n",
        "    metric = row['Metric']\n",
        "    all_score = row['All Features']\n",
        "    selected_score = row['Selected Features']\n",
        "    diff = row['Difference']\n",
        "\n",
        "    if abs(diff) < 0.01:\n",
        "        conclusion = \"≈ Similar performance\"\n",
        "    elif diff > 0:\n",
        "        conclusion = f\"✓ Selected features perform better by {diff:.3f}\"\n",
        "    else:\n",
        "        conclusion = f\"✗ All features perform better by {abs(diff):.3f}\"\n",
        "\n",
        "    print(f\"{metric}: All={all_score:.3f}, Selected={selected_score:.3f} - {conclusion}\")\n",
        "\n",
        "print(\"\\n2. FEATURE REDUCTION BENEFITS:\")\n",
        "print(f\"Number of features reduced from {X.shape[1]} to {len(selected_features)}\")\n",
        "print(f\"Feature reduction: {(1 - len(selected_features)/X.shape[1])*100:.1f}%\")\n",
        "print(\"\\nBenefits of feature selection:\")\n",
        "print(\"• Reduced model complexity\")\n",
        "print(\"• Faster training and prediction\")\n",
        "print(\"• Lower risk of overfitting\")\n",
        "print(\"• Better interpretability\")\n",
        "print(f\"• Selected features: {', '.join(selected_features)}\")\n",
        "\n",
        "print(\"\\n3. MODEL EFFICIENCY:\")\n",
        "print(f\"Selected features model uses only {len(selected_features)} features\")\n",
        "print(\"This makes the model more efficient and easier to interpret\")"
      ],
      "metadata": {
        "id": "AxxieYi2ljIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4: Experiment\n",
        "1. Experiment with different numbers of selected features (e.g., top 3, top 7).\n",
        "2. Discuss how feature selection affects model performance."
      ],
      "metadata": {
        "id": "5KZmkFt4lz2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PART 4: EXPERIMENT WITH DIFFERENT NUMBERS OF FEATURES\")\n",
        "\n",
        "# Experiment with different numbers of selected features\n",
        "feature_numbers = [3, 5, 7, 10]  # Try different numbers\n",
        "results_experiment = []\n",
        "\n",
        "for n_features in feature_numbers:\n",
        "    print(f\"\\n--- Experiment: Selecting top {n_features} features ---\")\n",
        "\n",
        "    # Apply RFE with different n_features\n",
        "    rfe_exp = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "                  n_features_to_select=n_features, step=1)\n",
        "    rfe_exp.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Get selected features\n",
        "    selected_mask = rfe_exp.support_\n",
        "    selected_features_exp = X.columns[selected_mask].tolist()\n",
        "\n",
        "    # Train model with these features\n",
        "    X_train_exp = X_train_scaled[:, selected_mask]\n",
        "    X_test_exp = X_test_scaled[:, selected_mask]\n",
        "\n",
        "    logreg_exp = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    logreg_exp.fit(X_train_exp, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_exp = logreg_exp.predict(X_test_exp)\n",
        "    y_pred_proba_exp = logreg_exp.predict_proba(X_test_exp)[:, 1]\n",
        "\n",
        "    accuracy_exp = accuracy_score(y_test, y_pred_exp)\n",
        "    f1_exp = f1_score(y_test, y_pred_exp)\n",
        "    roc_auc_exp = roc_auc_score(y_test, y_pred_proba_exp)\n",
        "\n",
        "    # Store results\n",
        "    results_experiment.append({\n",
        "        'n_features': n_features,\n",
        "        'selected_features': selected_features_exp,\n",
        "        'accuracy': accuracy_exp,\n",
        "        'f1_score': f1_exp,\n",
        "        'roc_auc': roc_auc_exp\n",
        "    })\n",
        "\n",
        "    print(f\"Selected features ({n_features}): {selected_features_exp}\")\n",
        "    print(f\"Accuracy: {accuracy_exp:.4f}, F1-Score: {f1_exp:.4f}, ROC-AUC: {roc_auc_exp:.4f}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results_experiment)\n",
        "print(\"\\n=== EXPERIMENT SUMMARY ===\")\n",
        "print(results_df[['n_features', 'accuracy', 'f1_score', 'roc_auc']].round(4))"
      ],
      "metadata": {
        "id": "znwjFwBklpiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the experiment results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Accuracy vs Number of Features\n",
        "axes[0].plot(results_df['n_features'], results_df['accuracy'],\n",
        "            marker='o', linestyle='-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Features')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Accuracy vs Number of Features')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xticks(results_df['n_features'])\n",
        "\n",
        "# Plot 2: F1-Score vs Number of Features\n",
        "axes[1].plot(results_df['n_features'], results_df['f1_score'],\n",
        "            marker='s', linestyle='-', linewidth=2, markersize=8, color='green')\n",
        "axes[1].set_xlabel('Number of Features')\n",
        "axes[1].set_ylabel('F1-Score')\n",
        "axes[1].set_title('F1-Score vs Number of Features')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xticks(results_df['n_features'])\n",
        "\n",
        "# Plot 3: ROC-AUC vs Number of Features\n",
        "axes[2].plot(results_df['n_features'], results_df['roc_auc'],\n",
        "            marker='^', linestyle='-', linewidth=2, markersize=8, color='red')\n",
        "axes[2].set_xlabel('Number of Features')\n",
        "axes[2].set_ylabel('ROC-AUC')\n",
        "axes[2].set_title('ROC-AUC vs Number of Features')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_xticks(results_df['n_features'])\n",
        "\n",
        "# Add horizontal line for all features performance\n",
        "axes[0].axhline(y=accuracy_all, color='gray', linestyle='--', alpha=0.7,\n",
        "               label=f'All features ({accuracy_all:.3f})')\n",
        "axes[1].axhline(y=f1_all, color='gray', linestyle='--', alpha=0.7,\n",
        "               label=f'All features ({f1_all:.3f})')\n",
        "axes[2].axhline(y=roc_auc_all, color='gray', linestyle='--', alpha=0.7,\n",
        "               label=f'All features ({roc_auc_all:.3f})')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle('Impact of Feature Selection on Model Performance', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aN-huOPtl2Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FINAL ANALYSIS AND CONCLUSIONS\")\n",
        "\n",
        "print(\"\\n1. OPTIMAL NUMBER OF FEATURES ANALYSIS:\")\n",
        "best_idx = results_df['accuracy'].idxmax()\n",
        "best_n = results_df.loc[best_idx, 'n_features']\n",
        "best_accuracy = results_df.loc[best_idx, 'accuracy']\n",
        "\n",
        "print(f\"The best accuracy ({best_accuracy:.4f}) was achieved with {best_n} features\")\n",
        "print(f\"Selected features for {best_n} features: {results_df.loc[best_idx, 'selected_features']}\")\n",
        "\n",
        "print(\"\\n2. FEATURE SELECTION IMPACT:\")\n",
        "print(\"• Model with all features: More complex, potentially overfitting\")\n",
        "print(\"• Model with selected features: Simpler, more interpretable\")\n",
        "print(\"• Performance difference is minimal, suggesting feature selection is effective\")\n",
        "\n",
        "print(\"\\n3. PRACTICAL IMPLICATIONS:\")\n",
        "print(\"✓ Using only 5 features instead of 9 reduces complexity by 44.4%\")\n",
        "print(\"✓ Model is faster to train and deploy\")\n",
        "print(\"✓ Easier to interpret and explain to non-technical stakeholders\")\n",
        "print(\"✓ Selected features provide insights into most important diagnostic factors\")\n",
        "\n",
        "print(\"\\n4. RECOMMENDATIONS:\")\n",
        "print(\"• For deployment: Use the model with selected features for efficiency\")\n",
        "print(\"• For maximum accuracy: Use all features if computational cost is not a concern\")\n",
        "print(\"• The top 5 selected features are likely the most clinically relevant\")\n",
        "\n",
        "print(\"\\n5. SELECTED FEATURES INTERPRETATION:\")\n",
        "print(\"The RFE selected these features as most important:\")\n",
        "for i, feature in enumerate(selected_features, 1):\n",
        "    # Provide clinical interpretation\n",
        "    interpretations = {\n",
        "        'Clump_Thickness': 'Thickness of cell clumps',\n",
        "        'Uniformity_Cell_Size': 'Consistency of cell sizes',\n",
        "        'Uniformity_Cell_Shape': 'Consistency of cell shapes',\n",
        "        'Marginal_Adhesion': 'How cells stick together',\n",
        "        'Single_Epithelial_Size': 'Size of epithelial cells',\n",
        "        'Bare_Nuclei': 'Presence of bare nuclei',\n",
        "        'Bland_Chromatin': 'Chromatin pattern',\n",
        "        'Normal_Nucleoli': 'Appearance of nucleoli',\n",
        "        'Mitoses': 'Cell division rate'\n",
        "    }\n",
        "    interpretation = interpretations.get(feature, 'Diagnostic feature')\n",
        "    print(f\"  {i}. {feature}: {interpretation}\")"
      ],
      "metadata": {
        "id": "LExr_4ecl9Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOFwp9V-mIyS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}